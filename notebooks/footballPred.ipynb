{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znSxzMaUMXoY"
   },
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aVFMY4UeMeY6"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "TOKEN = os.getenv(\"TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_W_WxZexMhwd"
   },
   "source": [
    "API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xL6q9n9cKsK2"
   },
   "outputs": [],
   "source": [
    "if TOKEN is None:\n",
    "    raise ValueError(\"API token not found\")\n",
    "\n",
    "HEADERS = {\"X-Auth-Token\": TOKEN}\n",
    "BASE = \"https://api.football-data.org/v4\"\n",
    "\n",
    "def get_matches(season):\n",
    "    url = f\"{BASE}/competitions/PL/matches?season={season}\"\n",
    "    r = requests.get(url, headers=HEADERS)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"matches\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnz4xj2DMmt1"
   },
   "source": [
    "Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EcMCP3RFMASQ"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for s in [2023]:\n",
    "    for m in get_matches(s):\n",
    "        if m[\"status\"] == \"FINISHED\":\n",
    "            data.append({\n",
    "                \"date\": m[\"utcDate\"],\n",
    "                \"home\": m[\"homeTeam\"][\"name\"],\n",
    "                \"away\": m[\"awayTeam\"][\"name\"],\n",
    "                \"hg\": m[\"score\"][\"fullTime\"][\"home\"],\n",
    "                \"ag\": m[\"score\"][\"fullTime\"][\"away\"]\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(data).sort_values(\"date\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tT9szUkGNOAL"
   },
   "source": [
    "Result encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tYkYiRm3NKZM"
   },
   "outputs": [],
   "source": [
    "def get_res(r):\n",
    "    if r.hg > r.ag:\n",
    "        return 0\n",
    "    elif r.hg < r.ag:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "df[\"res\"] = df.apply(get_res, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDW3RwcMNUzs"
   },
   "source": [
    "Team form features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "t4nnHX1eNNeR"
   },
   "outputs": [],
   "source": [
    "def teams_form_enhanced(df, w=5):\n",
    "    \"\"\"Rolling home/away form (last w games)\"\"\"\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "    teams = pd.unique(df[[\"home\", \"away\"]].values.ravel())\n",
    "    home_hist = {t: [] for t in teams}\n",
    "    away_hist = {t: [] for t in teams}\n",
    "    rows = []\n",
    "\n",
    "    for _, r in df.iterrows():\n",
    "        h, a = r.home, r.away\n",
    "        hf_home = home_hist[h][-w:]\n",
    "        af_away = away_hist[a][-w:]\n",
    "\n",
    "        rows.append({\n",
    "            \"h_gf\": np.mean([x[0] for x in hf_home]) if hf_home else 1.5,\n",
    "            \"h_ga\": np.mean([x[1] for x in hf_home]) if hf_home else 1.0,\n",
    "            \"a_gf\": np.mean([x[0] for x in af_away]) if af_away else 1.0,\n",
    "            \"a_ga\": np.mean([x[1] for x in af_away]) if af_away else 1.5,\n",
    "        })\n",
    "\n",
    "        home_hist[h].append((r.hg, r.ag))\n",
    "        away_hist[a].append((r.ag, r.hg))\n",
    "\n",
    "    return pd.concat([df, pd.DataFrame(rows)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-spo-134NqTQ"
   },
   "source": [
    "Team strength features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "H--UpTs9Ne8j"
   },
   "outputs": [],
   "source": [
    "def add_team_strength(df, historical_df):\n",
    "    \"\"\"Add PPG and goal difference features\"\"\"\n",
    "    team_stats = {}\n",
    "\n",
    "    for team in pd.unique(historical_df[[\"home\", \"away\"]].values.ravel()):\n",
    "        pts, gf, ga, games = 0, 0, 0, 0\n",
    "\n",
    "        home = historical_df[historical_df['home'] == team]\n",
    "        for _, r in home.iterrows():\n",
    "            games += 1\n",
    "            gf += r.hg\n",
    "            ga += r.ag\n",
    "            if r.hg > r.ag: pts += 3\n",
    "            elif r.hg == r.ag: pts += 1\n",
    "\n",
    "        away = historical_df[historical_df['away'] == team]\n",
    "        for _, r in away.iterrows():\n",
    "            games += 1\n",
    "            gf += r.ag\n",
    "            ga += r.hg\n",
    "            if r.ag > r.hg: pts += 3\n",
    "            elif r.ag == r.hg: pts += 1\n",
    "\n",
    "        ppg = pts / max(games, 1)\n",
    "        gd_per_game = (gf - ga) / max(games, 1)\n",
    "        team_stats[team] = {'ppg': ppg, 'gd': gd_per_game}\n",
    "\n",
    "    df['h_strength'] = df['home'].map(lambda x: team_stats.get(x, {'ppg': 1.0})['ppg']).fillna(1.0)\n",
    "    df['a_strength'] = df['away'].map(lambda x: team_stats.get(x, {'ppg': 1.0})['ppg']).fillna(1.0)\n",
    "    df['h_gd'] = df['home'].map(lambda x: team_stats.get(x, {'gd': 0})['gd']).fillna(0)\n",
    "    df['a_gd'] = df['away'].map(lambda x: team_stats.get(x, {'gd': 0})['gd']).fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = teams_form_enhanced(df)\n",
    "df = add_team_strength(df, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0t6mNbxN2hC"
   },
   "source": [
    "Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "l023Wx1NNy84"
   },
   "outputs": [],
   "source": [
    "feat = [\"h_gf\", \"h_ga\", \"a_gf\", \"a_ga\", \"h_strength\", \"a_strength\", \"h_gd\", \"a_gd\"]\n",
    "X = df[feat].values\n",
    "y = df[\"res\"].values\n",
    "\n",
    "split_idx = int(len(X) * 0.8)\n",
    "Xtr, Xva = X[:split_idx], X[split_idx:]\n",
    "ytr, yva = y[:split_idx], y[split_idx:]\n",
    "\n",
    "sc = StandardScaler()\n",
    "Xtr = sc.fit_transform(Xtr)\n",
    "Xva = sc.transform(Xva)\n",
    "\n",
    "class MatchDS(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZtOa8-aOEyV"
   },
   "source": [
    "Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "S_ZpLzHtN55z"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELZiUMr1OI8S"
   },
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "REBsFjKoOJEh",
    "outputId": "346103cb-0033-4af4-a15e-ef41c3836bff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 10 | Loss 0.9023\n",
      "Epoch 20 | Loss 0.8155\n",
      "Epoch 30 | Loss 0.7795\n",
      "Epoch 40 | Loss 0.7467\n",
      "Epoch 50 | Loss 0.7156\n",
      "Epoch 40 | Loss 0.7467\n",
      "Epoch 50 | Loss 0.7156\n"
     ]
    }
   ],
   "source": [
    "train_dl = DataLoader(MatchDS(Xtr, ytr), batch_size=32, shuffle=True)\n",
    "val_dl = DataLoader(MatchDS(Xva, yva), batch_size=256)\n",
    "\n",
    "model = Net(X.shape[1])\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Training model...\")\n",
    "for e in range(50):\n",
    "    model.train()\n",
    "    tot = 0\n",
    "    for xb, yb in train_dl:\n",
    "        opt.zero_grad()\n",
    "        loss = loss_fn(model(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        tot += loss.item()\n",
    "    if (e+1) % 10 == 0:\n",
    "        print(f\"Epoch {e+1} | Loss {tot/len(train_dl):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWx0Ptg8bvnH"
   },
   "source": [
    "Temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqU8BGQxbwFK",
    "outputId": "ca29477f-e953-4974-8713-1453000f9b82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal temperature: 1.747\n",
      "Log-loss before: 1.0108 | after: 0.9631\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def find_best_temperature(model, val_dl):\n",
    "    \"\"\"Find optimal temperature for probability calibration\"\"\"\n",
    "    model.eval()\n",
    "    all_logits, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            all_logits.append(model(xb))\n",
    "            all_labels.append(yb)\n",
    "\n",
    "    logits = torch.cat(all_logits)\n",
    "    labels = torch.cat(all_labels)\n",
    "\n",
    "    uncalibrated_probs = torch.softmax(logits, 1)\n",
    "    uncalibrated_loss = log_loss(labels.numpy(), uncalibrated_probs.numpy())\n",
    "\n",
    "    def temperature_loss(T):\n",
    "        scaled_probs = torch.softmax(logits / T, 1)\n",
    "        return log_loss(labels.numpy(), scaled_probs.numpy())\n",
    "\n",
    "    result = minimize_scalar(temperature_loss, bounds=(0.5, 3.0), method='bounded')\n",
    "    optimal_T = result.x\n",
    "\n",
    "    calibrated_probs = torch.softmax(logits / optimal_T, 1)\n",
    "    calibrated_loss = log_loss(labels.numpy(), calibrated_probs.numpy())\n",
    "\n",
    "    print(f\"Optimal temperature: {optimal_T:.3f}\")\n",
    "    print(f\"Log-loss before: {uncalibrated_loss:.4f} | after: {calibrated_loss:.4f}\")\n",
    "\n",
    "    return optimal_T\n",
    "\n",
    "optimal_temp = find_best_temperature(model, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_wL1pC8OU4I"
   },
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wn6uofp2OQDn",
    "outputId": "25f85c8a-d74a-42c7-def3-5da9930489ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.526\n",
      "Baseline (always home): 0.447\n",
      "Log Loss: 1.011\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Home       0.62      0.76      0.68        34\n",
      "        Away       0.65      0.46      0.54        28\n",
      "        Draw       0.07      0.07      0.07        14\n",
      "\n",
      "    accuracy                           0.53        76\n",
      "   macro avg       0.45      0.43      0.43        76\n",
      "weighted avg       0.53      0.53      0.52        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss, classification_report\n",
    "\n",
    "model.eval()\n",
    "preds, true, probs_list = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_dl:\n",
    "        logits = model(xb)\n",
    "        p = logits.argmax(1)\n",
    "        probs_list.extend(torch.softmax(logits, 1).numpy())\n",
    "        preds.extend(p.numpy())\n",
    "        true.extend(yb.numpy())\n",
    "\n",
    "acc = accuracy_score(true, preds)\n",
    "probs_array = np.vstack(probs_list)\n",
    "probs_array = probs_array / probs_array.sum(axis=1, keepdims=True)\n",
    "logloss = log_loss(true, probs_array)\n",
    "\n",
    "baseline_preds = [0] * len(true)\n",
    "baseline_acc = accuracy_score(true, baseline_preds)\n",
    "\n",
    "print(f\"Validation Accuracy: {acc:.3f}\")\n",
    "print(f\"Baseline (always home): {baseline_acc:.3f}\")\n",
    "print(f\"Log Loss: {logloss:.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true, preds, target_names=[\"Home\", \"Away\", \"Draw\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cI9bDoiEaqvH"
   },
   "source": [
    "Calibration analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vekxXdo9as6S",
    "outputId": "c9497c04-02c7-4eb4-d9a0-e2034d87dbf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration Analysis:\n",
      "Home: predicted avg 0.457, actual rate 0.447\n",
      "Away: predicted avg 0.288, actual rate 0.368\n",
      "Draw: predicted avg 0.255, actual rate 0.184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "print(\"Calibration Analysis:\")\n",
    "for i, outcome in enumerate([\"Home\", \"Away\", \"Draw\"]):\n",
    "    y_binary = (np.array(true) == i).astype(int)\n",
    "    prob_pred = probs_array[:, i]\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        y_binary, prob_pred, n_bins=5, strategy='quantile'\n",
    "    )\n",
    "    print(f\"{outcome}: predicted avg {prob_pred.mean():.3f}, actual rate {y_binary.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oxqAYShOaOE"
   },
   "source": [
    "Load current season data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-Jvt98qOako",
    "outputId": "c50f578e-1285-4863-c1be-005f447a016c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Played: 210 | Future: 170\n"
     ]
    }
   ],
   "source": [
    "data_25 = []\n",
    "for m in get_matches(2025):\n",
    "    data_25.append({\n",
    "        \"date\": m[\"utcDate\"],\n",
    "        \"home\": m[\"homeTeam\"][\"name\"],\n",
    "        \"away\": m[\"awayTeam\"][\"name\"],\n",
    "        \"hg\": m[\"score\"][\"fullTime\"][\"home\"],\n",
    "        \"ag\": m[\"score\"][\"fullTime\"][\"away\"],\n",
    "        \"status\": m[\"status\"]\n",
    "    })\n",
    "\n",
    "df25 = pd.DataFrame(data_25).sort_values(\"date\").reset_index(drop=True)\n",
    "played = df25[df25.status == \"FINISHED\"].copy()\n",
    "future = df25[df25.status == \"TIMED\"].copy()\n",
    "\n",
    "print(f\"\\nPlayed: {len(played)} | Future: {len(future)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNFrLTi1OiFZ"
   },
   "source": [
    "Feature prep for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0ZoVfXOIOcC-"
   },
   "outputs": [],
   "source": [
    "base_cols = ['date', 'home', 'away', 'hg', 'ag']\n",
    "df_base = df[base_cols].copy()\n",
    "played_base = played[base_cols].copy()\n",
    "\n",
    "future_temp = future[['date', 'home', 'away']].copy()\n",
    "future_temp['hg'] = 0\n",
    "future_temp['ag'] = 0\n",
    "\n",
    "all_games = pd.concat([df_base, played_base, future_temp], ignore_index=True)\n",
    "all_games = teams_form_enhanced(all_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJVBCy0POjI1"
   },
   "source": [
    "Add team strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VPU9pH1zOhUm"
   },
   "outputs": [],
   "source": [
    "all_historical = pd.concat([df[base_cols], played_base], ignore_index=True)\n",
    "all_games = add_team_strength(all_games, all_historical)\n",
    "\n",
    "future_feat = all_games.iloc[-len(future):][feat].values\n",
    "future_feat = sc.transform(future_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpSW1_1rOoTj"
   },
   "source": [
    "Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jaKdyifTOobn"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(torch.tensor(future_feat, dtype=torch.float32))\n",
    "    probs = torch.softmax(logits / optimal_temp, 1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_Hqmu8jOtvh"
   },
   "source": [
    "Prediction summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-23euKmOsAn",
    "outputId": "98abdce4-4229-4884-b792-26406c4a1af0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg probabilities - Home: 0.400, Away: 0.480, Draw: 0.120\n"
     ]
    }
   ],
   "source": [
    "future[\"p_home\"] = probs[:,0]\n",
    "future[\"p_away\"] = probs[:,1]\n",
    "future[\"p_draw\"] = probs[:,2]\n",
    "\n",
    "print(f\"Avg probabilities - Home: {future['p_home'].mean():.3f}, Away: {future['p_away'].mean():.3f}, Draw: {future['p_draw'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogYiGBdCPEUa"
   },
   "source": [
    "Monte Carlo simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vvaqvh88PEgE",
    "outputId": "6deebcdf-7a52-467f-81ab-4dc7e0546d45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1000 simulations...\n",
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "teams = pd.unique(df25[[\"home\",\"away\"]].values.ravel())\n",
    "points = {t:0 for t in teams}\n",
    "\n",
    "for _, r in played.iterrows():\n",
    "    if r.hg > r.ag: points[r.home] += 3\n",
    "    elif r.hg < r.ag: points[r.away] += 3\n",
    "    else:\n",
    "        points[r.home] += 1\n",
    "        points[r.away] += 1\n",
    "\n",
    "def simulate():\n",
    "    \"\"\"Simulate remaining season with noise and momentum\"\"\"\n",
    "    pts = points.copy()\n",
    "    team_form = {t: 0 for t in teams}\n",
    "    NOISE_STD = 0.20\n",
    "    FORM_IMPACT = 0.05\n",
    "\n",
    "    for _, r in future.iterrows():\n",
    "        probs = np.array([r.p_home, r.p_away, r.p_draw])\n",
    "        probs = probs / probs.sum()\n",
    "\n",
    "        noise = np.random.normal(0, NOISE_STD, 3)\n",
    "        probs[0] += team_form[r.home] * FORM_IMPACT\n",
    "        probs[1] += team_form[r.away] * FORM_IMPACT\n",
    "        probs = np.maximum(probs + noise, 0.05)\n",
    "        probs = probs / probs.sum()\n",
    "\n",
    "        res = np.random.choice([0, 1, 2], p=probs)\n",
    "\n",
    "        if res == 0:\n",
    "            pts[r.home] += 3\n",
    "            team_form[r.home] = min(team_form[r.home] + 1, 3)\n",
    "            team_form[r.away] = max(team_form[r.away] - 1, -3)\n",
    "        elif res == 1:\n",
    "            pts[r.away] += 3\n",
    "            team_form[r.away] = min(team_form[r.away] + 1, 3)\n",
    "            team_form[r.home] = max(team_form[r.home] - 1, -3)\n",
    "        else:\n",
    "            pts[r.home] += 1\n",
    "            pts[r.away] += 1\n",
    "            team_form[r.home] *= 0.5\n",
    "            team_form[r.away] *= 0.5\n",
    "\n",
    "    return pts\n",
    "\n",
    "cumulative_points = {t: 0 for t in teams}\n",
    "title_wins = {t: 0 for t in teams}\n",
    "\n",
    "n_sims = 1000\n",
    "print(f\"Running {n_sims} simulations...\")\n",
    "for i in range(n_sims):\n",
    "    sim_points = simulate()\n",
    "    winner = max(sim_points, key=sim_points.get)\n",
    "    title_wins[winner] += 1\n",
    "    for team in teams:\n",
    "        cumulative_points[team] += sim_points[team]\n",
    "\n",
    "avg_points = {t: cumulative_points[t] / n_sims for t in teams}\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdiZ2REkPNPR"
   },
   "source": [
    "Final table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iaqwnPgwPNcr",
    "outputId": "8729e82e-d155-4a0c-87c0-80bfc7f5c0e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================================================================================================\n",
      "ðŸ“Š PREDICTED FINAL PREMIER LEAGUE TABLE 2025/26\n",
      "==============================================================================================================\n",
      "                          Team   P   W  D   L   GF   GA    GD  Curr  Proj  Title%\n",
      "1                   Arsenal FC  21  15  4   2 40.0 14.0  26.0    49  80.4    71.8\n",
      "2               Aston Villa FC  21  13  4   4 33.0 24.0   9.0    43  72.4    15.3\n",
      "3           Manchester City FC  21  13  4   4 45.0 19.0  26.0    43  72.1    12.4\n",
      "4                 Liverpool FC  21  10  5   6 32.0 28.0   4.0    35  62.4     0.5\n",
      "5                 Brentford FC  21  10  3   8 35.0 28.0   7.0    33  56.2     0.0\n",
      "6         Manchester United FC  21   8  8   5 36.0 32.0   4.0    32  55.0     0.0\n",
      "7                   Chelsea FC  21   8  7   6 34.0 24.0  10.0    31  54.7     0.0\n",
      "8               Sunderland AFC  21   7  9   5 21.0 22.0  -1.0    30  54.6     0.0\n",
      "9                    Fulham FC  21   9  4   8 30.0 30.0   0.0    31  54.0     0.0\n",
      "10           Crystal Palace FC  21   7  7   7 22.0 23.0  -1.0    28  53.9     0.0\n",
      "11        Tottenham Hotspur FC  21   7  6   8 30.0 27.0   3.0    27  53.0     0.0\n",
      "12             AFC Bournemouth  21   6  8   7 34.0 40.0  -6.0    26  52.3     0.0\n",
      "13   Brighton & Hove Albion FC  21   7  8   6 31.0 28.0   3.0    29  52.0     0.0\n",
      "14         Newcastle United FC  21   9  5   7 32.0 27.0   5.0    32  51.5     0.0\n",
      "15                  Everton FC  21   8  5   8 23.0 25.0  -2.0    29  49.0     0.0\n",
      "16             Leeds United FC  21   5  7   9 29.0 37.0  -8.0    22  45.5     0.0\n",
      "17        Nottingham Forest FC  21   6  3  12 21.0 34.0 -13.0    21  43.8     0.0\n",
      "18          West Ham United FC  21   3  5  13 22.0 43.0 -21.0    14  34.3     0.0\n",
      "19                  Burnley FC  21   3  4  14 22.0 41.0 -19.0    13  31.3     0.0\n",
      "20  Wolverhampton Wanderers FC  21   1  4  16 15.0 41.0 -26.0     7  29.2     0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "team_stats = {}\n",
    "for team in teams:\n",
    "    team_stats[team] = {\n",
    "        'played': 0, 'wins': 0, 'draws': 0, 'losses': 0,\n",
    "        'gf': 0, 'ga': 0\n",
    "    }\n",
    "\n",
    "for _, r in played.iterrows():\n",
    "    team_stats[r.home]['played'] += 1\n",
    "    team_stats[r.home]['gf'] += r.hg\n",
    "    team_stats[r.home]['ga'] += r.ag\n",
    "\n",
    "    team_stats[r.away]['played'] += 1\n",
    "    team_stats[r.away]['gf'] += r.ag\n",
    "    team_stats[r.away]['ga'] += r.hg\n",
    "\n",
    "    if r.hg > r.ag:\n",
    "        team_stats[r.home]['wins'] += 1\n",
    "        team_stats[r.away]['losses'] += 1\n",
    "    elif r.hg < r.ag:\n",
    "        team_stats[r.away]['wins'] += 1\n",
    "        team_stats[r.home]['losses'] += 1\n",
    "    else:\n",
    "        team_stats[r.home]['draws'] += 1\n",
    "        team_stats[r.away]['draws'] += 1\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Team': teams,\n",
    "    'P': [team_stats[t]['played'] for t in teams],\n",
    "    'W': [team_stats[t]['wins'] for t in teams],\n",
    "    'D': [team_stats[t]['draws'] for t in teams],\n",
    "    'L': [team_stats[t]['losses'] for t in teams],\n",
    "    'GF': [team_stats[t]['gf'] for t in teams],\n",
    "    'GA': [team_stats[t]['ga'] for t in teams],\n",
    "    'GD': [team_stats[t]['gf'] - team_stats[t]['ga'] for t in teams],\n",
    "    'Curr': [points[t] for t in teams],\n",
    "    'Proj': [avg_points[t] for t in teams],\n",
    "    'Title%': [title_wins[t] / n_sims * 100 for t in teams]\n",
    "}).sort_values('Proj', ascending=False).reset_index(drop=True)\n",
    "\n",
    "results_df.index = results_df.index + 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*110)\n",
    "print(\"ðŸ“Š PREDICTED FINAL PREMIER LEAGUE TABLE 2025/26\")\n",
    "print(\"=\"*110)\n",
    "print(results_df.to_string(index=True, float_format=lambda x: f'{x:.1f}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving outputs\n",
      "Saved to ../outputs/ : table, metrics, confusion matrix, calibration\n",
      "Saved to ../outputs/ : table, metrics, confusion matrix, calibration\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "output_dir = '../outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "print(\"Saving outputs\")\n",
    "\n",
    "# Predicted table\n",
    "results_df.to_csv(f'{output_dir}/predicted_table_{timestamp}.csv', index=True)\n",
    "\n",
    "# Metrics\n",
    "metrics = {\n",
    "    'timestamp': timestamp,\n",
    "    'accuracy': float(acc),\n",
    "    'log_loss': float(logloss),\n",
    "    'baseline_accuracy': float(baseline_acc),\n",
    "    'temperature': float(optimal_temp),\n",
    "    'n_simulations': n_sims\n",
    "}\n",
    "with open(f'{output_dir}/metrics_{timestamp}.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(true, preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Home', 'Away', 'Draw'],\n",
    "            yticklabels=['Home', 'Away', 'Draw'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.savefig(f'{output_dir}/confusion_matrix_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Calibration plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, outcome in enumerate([\"Home\", \"Away\", \"Draw\"]):\n",
    "    y_bin = (np.array(true) == i).astype(int)\n",
    "    frac, mean_pred = calibration_curve(y_bin, probs_array[:, i], n_bins=5, strategy='quantile')\n",
    "    plt.plot(mean_pred, frac, 'o-', label=outcome)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Perfect')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Actual Frequency')\n",
    "plt.title('Calibration Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(f'{output_dir}/calibration_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"Saved to {output_dir}/ : table, metrics, confusion matrix, calibration\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNg+Jc2xXKdbJr2kcLT/Gg3",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
